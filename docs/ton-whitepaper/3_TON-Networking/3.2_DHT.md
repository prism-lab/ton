# TON DHT: Kademlia-like Distributed Hash Table
The {\em TON Distributed Hash Table (DHT)\/} plays a crucial role in
the networking part of the TON Project, being used to locate other
nodes in the network. For example, a client wanting to commit a
transaction into a shardchain might want to find a validator or a
collator of that shardchain, or at least some node that might relay
the client's transaction to a collator. This can be done by looking up
a special key in the TON DHT. Another important application of the TON
DHT is that it can be used to quickly populate a new node's neighbor
table (cf.~\ptref{sp:net.startup}), simply by looking up a random key,
or the new node's address. If a node uses proxying and tunneling for
its inbound datagrams, it publishes the tunnel identifier and its
entry point (e.g., IP address and UDP port) in the TON DHT; then all
nodes wishing to send datagrams to that node will obtain this contact
information from the DHT first.

The TON DHT is a member of the family of {\em Kademlia-like distributed
  hash tables\/}~\cite{Kademlia}.

# \nxsubpoint \embt(Keys of the TON DHT.)  The {\em keys\/} of the TON
DHT are simply 256-bit integers. In most cases, they are computed as
$\Sha$ of a TL-serialized object (cf.~\ptref{sp:TL}), called {\em
  preimage\/} of the key, or {\em key description}. In some cases, the
abstract addresses of the TON Network nodes (cf.~\ptref{sp:abs.addr})
can also be used as keys of the TON DHT, because they are also
256-bit, and they are also hashes of TL-serialized objects. For
example, if a node is not afraid of publishing its IP address, it can
be found by anybody who knows its abstract address by simply looking
up that address as a key in the DHT.

# \nxsubpoint \embt(Values of the DHT.)  The {\em values\/} assigned to
these 256-bit keys are essentially arbitrary byte strings of limited
length. The interpretation of such byte strings is determined by the
preimage of the corresponding key; it is usually known both by the
node that looks up the key, and by the node that stores the key.

# \nxsubpoint \embt(Nodes of the DHT. Semi-permanent network
identities.)  The key-value mapping of the TON DHT is kept on the {\em
  nodes\/} of the DHT---essentially, all members of the TON
Network. To this end, any node of the TON Network (perhaps with the
exception of some very light nodes), apart from any number of
ephemeral and permanent abstract addresses described
in~\ptref{sp:abs.addr}, has at least one ``semi-permanent address'',
which identifies it as a member of the TON DHT. This {\em
  semi-permanent\/} or {\em DHT address\/} should not to be changed
too often, otherwise other nodes would be unable to locate the keys
they are looking for. If a node does not want to reveal its ``true''
identity, it generates a separate abstract address to be used only for
the purpose of participating in the DHT. However, this abstract
address must be public, because it will be associated with the node's
IP address and port.

# \nxsubpoint \embt(Kademlia distance.)  Now we have both 256-bit keys
and 256-bit (semi-permanent) node addresses. We introduce the
so-called {\em XOR distance\/} or {\em Kademlia distance~$d_K$} on the
set of 256-bit sequences, given by
\begin{equation}
  d_K(x,y):=(x\oplus y)\quad\text{interpreted as an unsigned 256-bit
    integer}
\end{equation}
Here $x\oplus y$ denotes the bitwise eXclusive OR (XOR) of two bit
sequences of the same length.

The Kademlia distance introduces a metric on the set $\st2^{256}$ of
all 256-bit sequences. In particular, we have $d_K(x,y)=0$ if and only
if $x=y$, $d_K(x,y)=d_K(y,x)$, and $d_K(x,z)\leq
d_K(x,y)+d_K(y,z)$. Another important property is that {\em there is
  only one point at any given distance from~$x$}: $d_K(x,y)=d_K(x,y')$
implies $y=y'$.

# \nxsubpoint \embt(Kademlia-like DHTs and the TON DHT.)  We say that a
distributed hash table (DHT) with 256-bit keys and 256-bit node
addresses is a {\em Kademlia-like DHT\/} if it is expected to keep the
value of key $K$ on $s$ Kademlia-nearest nodes to $K$ (i.e., the $s$
nodes with smallest Kademlia distance from their addresses to $K$.)

Here $s$ is a small parameter, say, $s=7$, needed to improve
reliability of the DHT (if we would keep the key only on one node, the
nearest one to~$K$, the value of that key would be lost if that only
node goes offline).

The TON DHT is a Kademlia-like DHT, according to this definition. It
is implemented over the ADNL protocol described in~\ptref{sect:ANL}.

# \nxsubpoint \embt(Kademlia routing table.)  Any node participating in
a Kademlia-like DHT usually maintains a {\em Kademlia routing
  table}. In the case of TON DHT, it consists of $n=256$ buckets,
numbered from $0$ to $n-1$. The $i$-th bucket will contain information
about some known nodes (a fixed number $t$ of ``best'' nodes, and
maybe some extra candidates) that lie at a Kademlia distance from
$2^i$ to $2^{i+1}-1$ from the node's address $a$.\footnote{If there
  are sufficiently many nodes in a bucket, it can be subdivided
  further into, say, eight sub-buckets depending on the top four bits
  of the Kademlia distance. This would speed up DHT lookups.} This
information includes their (semi-permanent) addresses, IP addresses
and UDP ports, and some availability information such as the time and
the delay of the last ping.

When a Kademlia node learns about any other Kademlia node as a result
of some query, it includes it into a suitable bucket of its routing
table, first as a candidate. Then, if some of the ``best'' nodes in
that bucket fail (e.g., do not respond to ping queries for a long
time), they can be replaced by some of the candidates. In this way the
Kademlia routing table stays populated.

New nodes from the Kademlia routing table are also included in the
ADNL neighbor table described in~\ptref{sp:net.startup}. If a ``best''
node from a bucket of the Kademlia routing table is used often, a
channel in the sense described in~\ptref{sp:net.channels} can be
established to facilitate the encryption of datagrams.

A special feature of the TON DHT is that it tries to select nodes with
the smallest round-trip delays as the ``best'' nodes for the buckets
of the Kademlia routing table.

# \nxsubpoint (Kademlia network queries.)  A Kademlia node usually
supports the following network queries:
\begin{itemize}
\item $\Ping$ -- Checks node availability.
\item $\Store(key,value)$ -- Asks the node to keep $value$ as a value
  for key $key$. For TON DHT, the $\Store$ queries are slightly more
  complicated (cf.~\ptref{sp:DHT.store}).
\item $\FindNode(key,l)$ -- Asks the node to return $l$
  Kademlia-nearest known nodes (from its Kademlia routing table) to
  $key$.
\item $\FindValue(key,l)$ -- The same as above, but if the node knows
  the value corresponding to key $key$, it just returns that value.
\end{itemize}

When any node wants to look up the value of a key $K$, it first
creates a set $S$ of $s'$ nodes (for some small value of $s'$, say,
$s'=5$), nearest to $K$ with respect to the Kademlia distance among
all known nodes (i.e., they are taken from the Kademlia routing
table). Then a $\FindValue$ query is sent to each of them, and nodes
mentioned in their answers are included in $S$. Then the $s'$ nodes
from $S$, nearest to $K$, are also sent a $\FindValue$ query if this
hasn't been done before, and the process continues until the value is
found or the set $S$ stops growing. This is a sort of ``beam search''
of the node nearest to $K$ with respect to Kademlia distance.

If the value of some key $K$ is to be set, the same procedure is run
for $s'\geq s$, with $\FindNode$ queries instead of $\FindValue$, to
find $s$ nearest nodes to $K$. Afterwards, $\Store$ queries are sent
to all of them.

There are some less important details in the implementation of a
Kademlia-like DHT (for example, any node should look up $s$ nearest
nodes to itself, say, once every hour, and re-publish all stored keys
to them by means of $\Store$ queries). We will ignore them for the
time being.

# \nxsubpoint \embt(Booting a Kademlia node.)  When a Kademlia node goes
online, it first populates its Kademlia routing table by looking up
its own address. During this process, it identifies the $s$ nearest
nodes to itself. It can download from them all $(key,value)$ pairs
known to them to populate its part of the DHT.

# \nxsubpoint\label{sp:DHT.store} \embt(Storing values in TON DHT.)
Storing values in TON DHT is slightly different from a general
Kademlia-like DHT. When someone wishes to store a value, she must
provide not only the key $K$ itself to the $\Store$ query, but also
its {\em preimage\/}---i.e., a TL-serialized string (with one of
several predefined TL-constructors at the beginning) containing a
``description'' of the key. This key description is later kept by the
node, along with the key and the value.

The key description describes the ``type'' of the object being stored,
its ``owner'', and its ``update rules'' in case of future updates. The
owner is usually identified by a public key included in the key
description. If it is included, normally only updates signed by the
corresponding private key will be accepted. The ``type'' of the stored
object is normally just a byte string. However, in some cases it can
be more sophisticated---for example, an input tunnel description
(cf.~\ptref{sp:tunnels}), or a collection of node addresses.

The ``update rules'' can also be different. In some cases, they simply
permit replacing the old value with the new value, provided the new
value is signed by the owner (the signature must be kept as part of
the value, to be checked later by any other nodes after they obtain
the value of this key). In other cases, the old value somehow affects
the new value. For example, it can contain a sequence number, and the
old value is overwritten only if the new sequence number is larger (to
prevent replay attacks).

# \nxsubpoint\label{sp:distr.torr.tr} \embt(Distributed ``torrent
trackers'' and ``network interest groups'' in TON DHT.)  Yet another
interesting case is when the value contains a list of nodes---perhaps
with their IP addresses and ports, or just with their abstract
addresses---and the ``update rule'' consists in including the
requester in this list, provided she can confirm her identity.

This mechanism might be used to create a distributed ``torrent
tracker'', where all nodes interested in a certain ``torrent'' (i.e.,
a certain file) can find other nodes that are interested in the same
torrent, or already have a copy.

{\em TON Storage\/} (cf.~\ptref{sp:ex.ton.storage}) uses this
technology to find the nodes that have a copy of a required file
(e.g., a snapshot of the state of a shardchain, or an old
block). However, its more important use is to create ``overlay
multicast subnetworks'' and ``network interest groups''
(cf.~\ptref{sect:overlay}). The idea is that only some nodes are
interested in the updates of a specific shardchain. If the number of
shardchains becomes very large, finding even one node interested in
the same shard may become complicated. This ``distributed torrent
tracker'' provides a convenient way to find some of these
nodes. Another option would be to request them from a validator, but
this would not be a scalable approach, and validators might choose not
to respond to such queries coming from arbitrary unknown nodes.

# \nxsubpoint \embt(Fall-back keys.)  Most of the ``key types''
described so far have an extra 32-bit integer field in their TL
description, normally equal to zero. However, if the key obtained by
hashing that description cannot be retrieved from or updated in the
TON DHT, the value in this field is increased, and a new attempt is
made. In this way, one cannot ``capture'' and ``censor'' a key (i.e.,
perform a key retention attack) by creating a lot of abstract
addresses lying near the key under attack and controlling the
corresponding DHT nodes.

# \nxsubpoint\label{sp:loc.serv} \embt(Locating services.)  Some
services, located in the TON Network and available through the
(higher-level protocols built upon the) TON ADNL described
in~\ptref{sect:ANL}, may want to publish their abstract addresses
somewhere, so that their clients would know where to find them.

However, publishing the service's abstract address in the TON
Blockchain may not be the best approach, because the abstract address
might need to be changed quite often, and because it could make sense
to provide several addresses, for reliability or load balancing
purposes.

An alternative is to publish a public key into the TON Blockchain, and
use a special DHT key indicating that public key as its ``owner'' in
the TL description string (cf.~\ptref{sp:TL}) to publish an up-to-date
list of the service's abstract addresses. This is one of the
approaches exploited by TON Services.

# \nxsubpoint \embt(Locating owners of TON blockchain accounts.)  In
most cases, owners of TON blockchain accounts would not like to be
associated with abstract network addresses, and especially IP
addresses, because this can violate their privacy. In some cases,
however, the owner of a TON blockchain account may want to publish
one or several abstract addresses where she could be contacted.

A typical case is that of a node in the TON Payments ``lightning
network'' (cf.~\ptref{sect:lightning}), the platform for instant
cryptocurrency transfers. A public TON Payments node may want not only
to establish payment channels with other peers, but also to publish an
abstract network address that could be used to contact it at a later
time for transferring payments along the already-established channels.

One option would be to include an abstract network address in the
smart contract creating the payment channel. A more flexible option is
to include a public key in the smart contract, and then use DHT as
explained in~\ptref{sp:loc.serv}.

The most natural way would be to use the same private key that
controls the account in the TON Blockchain to sign and publish updates
in the TON DHT about the abstract addresses associated with that
account. This is done almost in the same way as described
in~\ptref{sp:loc.serv}; however, the DHT key employed would require a
special key description, containing only the $\accountid$ itself,
equal to $\Sha$ of the ``account description'', which contains the
public key of the account. The signature, included in the value of
this DHT key, would contain the account description as well.

In this way, a mechanism for locating abstract network addresses of
some owners of the TON Blockchain accounts becomes available.

# \nxsubpoint\label{sp:loc.abs.addr} \embt(Locating abstract addresses.)
Notice that the TON DHT, while being implemented over TON ADNL, is
itself used by the TON ADNL for several purposes.

The most important of them is to locate a node or its contact data
starting from its 256-bit abstract address. This is necessary because
the TON ADNL should be able to send datagrams to arbitrary 256-bit
abstract addresses, even if no additional information is provided.

To this end, the 256-bit abstract address is simply looked up as a key
in the DHT. Either a node with this address (i.e., using this address
as a public semi-persistent DHT address) is found, in which case its
IP address and port can be learned; or, an input tunnel description
may be retrieved as the value of the key in question, signed by the
correct private key, in which case this tunnel description would be
used to send ADNL datagrams to the intended recipient.

Notice that in order to make an abstract address ``public'' (reachable
from any nodes in the network), its owner must either use it as a
semi-permanent DHT address, or publish (in the DHT key equal to the
abstract address under consideration) an input tunnel description with
another of its public abstract addresses (e.g., the semi-permanent
address) as the tunnel's entry point. Another option would be to
simply publish its IP address and UDP port.
